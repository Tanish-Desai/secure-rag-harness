# Secure RAG Evaluation Harness

This repository contains the reproducibility harness for the paper **"Secure RAG Engineering"**. It allows engineering teams to benchmark defenses (EcoSafeRAG, ATM, Guardrails) against attacks (Poisoning, Prompt Injection) under strict SLA constraints.

## Architecture
The system follows a microservices architecture running in Docker:
1.  **Gateway:** API Entry point and Topology Router.
2.  **Retriever:** Hybrid search engine (BM25 + Dense).
3.  **Vector DB:** PostgreSQL with `pgvector` (Pinned Version).
4.  **Policy:** Middleware for input/output guardrails.
5.  **Logger:** Centralized telemetry sink for metrics.

## Recommended Environment

### Hardware
* **OS:** Ubuntu 22.04 (WSL2 on Windows or Native Linux).
* **GPU:** NVIDIA GPU with >= 6GB VRAM recommended (Tested on RTX 4060).
* **RAM:** 16GB System RAM minimum.

### Software Tools
* **Docker Desktop** (Enable WSL2 Integration).
* **Ollama** (Linux Version).
* **Make** (`sudo apt install make`).

---

## Quick Start

### 1. Download Model Weights
To ensure exact reproducibility, we do not pull "latest" models. We manually download a specific commit hash of **Llama-3-8B-Instruct (Quantized)**.

```bash
# Create directory
mkdir -p services/llm/weights

# Download GGUF (4.7GB) - Pinned to Commit 86e0c07
wget -c --show-progress \
  "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/86e0c07efa3f1b6f06ea13e31b1e930dce865ae4/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf?download=true" \
  -O services/llm/weights/llama3.gguf
````

### 2\. Setup & Install

Run the setup command. This will:

1.  Generate your `.env` configuration file.
2.  Generate a custom `Modelfile` using your absolute file paths.
3.  Register the `secure-rag-llama3` model in Ollama.



```bash
make setup
```

**‚ö†Ô∏è Important for WSL2 Users:**
The `.env` file generated by `make setup` defaults to a standard host URL. You **must** edit `.env` to point to your specific WSL IP address so Docker containers can reach Ollama.

1.  Find your IP: `ip addr show eth0`
2.  Edit `.env`: `LLM_API_BASE=http://172.x.x.x:11434/v1`

### 3\. Serve the Model

In a separate terminal (Terminal A), start the Ollama server.

```bash
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

### 4\. Launch Infrastructure

In your main terminal (Terminal B), build and start the container stack.

```bash
make up
```

*First run will take a few minutes to download the pinned Docker images.*

### 5\. Run Smoke Test

Verify the pipeline is connected and generating answers.

```bash
make test
```

**Expected Output:**

```json
{"response": "Based on the provided context...", "context_used": [...], "model": "secure-rag-llama3"}
```

-----

## üß™ Running Experiments

To run a full evaluation suite as described in the paper:

```bash
make run attack=prompt_injection defense=guardrails profile=P1
```

  * **Attacks:** `none`, `prompt_injection`, `retrieval_poisoning`, `opinion_manipulation`
  * **Defenses:** `baseline`, `guardrails`, `ecosafe`, `atm`, `skeptical`
  * **Profiles:** `P1` (SaaS), `P2` (VPC), `P3` (Regulated)

## üì¶ Reproducibility Notes

  * **LLM:** Llama-3-8B (Q4\_K\_M) pinned to Git Commit `86e0c07`.
  * **Dense Embedding:** `all-MiniLM-L6-v2` pinned to Git Commit `c9745ed`.
  * **Sparse Retrieval:** `rank_bm25` v0.2.2 (Algorithmic Pin).
  * **Vector DB:** `pgvector/pgvector:pg16` (Pinned Image).
  * **Base Image:** `python:3.11-slim-bookworm` (Debian 12).
